{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Evolutionary\n",
    "using Flux\n",
    "using Flux: onehot, onecold, logitcrossentropy #, throttle, @epochs\n",
    "using MLDatasets\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using Iris dataset for this exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 150)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = Iris.features();\n",
    "slabels = Iris.labels();\n",
    "classes = unique(slabels)  # unique classes in the dataset\n",
    "nclasses = length(classes) # number of classes\n",
    "d, n = size(features)          # dimension and size if the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert feature and labels in appropriate for training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Vector{Tuple{SubArray{Float64, 1, Matrix{Float64}, Tuple{Base.Slice{Base.OneTo{Int64}}, Int64}, true}, Flux.OneHotArray{UInt32, 3, 0, 1, UInt32}}}:\n",
       " ([5.1, 3.5, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.9, 3.0, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.7, 3.2, 1.3, 0.2], [1, 0, 0])\n",
       " ([4.6, 3.1, 1.5, 0.2], [1, 0, 0])\n",
       " ([5.0, 3.6, 1.4, 0.2], [1, 0, 0])\n",
       " ([5.4, 3.9, 1.7, 0.4], [1, 0, 0])\n",
       " ([4.6, 3.4, 1.4, 0.3], [1, 0, 0])\n",
       " ([5.0, 3.4, 1.5, 0.2], [1, 0, 0])\n",
       " ([4.4, 2.9, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.9, 3.1, 1.5, 0.1], [1, 0, 0])\n",
       " ([5.4, 3.7, 1.5, 0.2], [1, 0, 0])\n",
       " ([4.8, 3.4, 1.6, 0.2], [1, 0, 0])\n",
       " ([4.8, 3.0, 1.4, 0.1], [1, 0, 0])\n",
       " ⋮\n",
       " ([6.0, 3.0, 4.8, 1.8], [0, 0, 1])\n",
       " ([6.9, 3.1, 5.4, 2.1], [0, 0, 1])\n",
       " ([6.7, 3.1, 5.6, 2.4], [0, 0, 1])\n",
       " ([6.9, 3.1, 5.1, 2.3], [0, 0, 1])\n",
       " ([5.8, 2.7, 5.1, 1.9], [0, 0, 1])\n",
       " ([6.8, 3.2, 5.9, 2.3], [0, 0, 1])\n",
       " ([6.7, 3.3, 5.7, 2.5], [0, 0, 1])\n",
       " ([6.7, 3.0, 5.2, 2.3], [0, 0, 1])\n",
       " ([6.3, 2.5, 5.0, 1.9], [0, 0, 1])\n",
       " ([6.5, 3.0, 5.2, 2.0], [0, 0, 1])\n",
       " ([6.2, 3.4, 5.4, 2.3], [0, 0, 1])\n",
       " ([5.9, 3.0, 5.1, 1.8], [0, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ (x, onehot(l, classes)) for (x, l) in zip(eachcol(features), slabels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some auxiliary functions: model accuracy and its loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 3 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,x,y) = sum(onecold(model(x)) .== onecold(y))/size(x,2)\n",
    "accuracy(xy, model) = sum( onecold(model(x)) .== onecold(y) for (x,y) in xy) /length(xy)\n",
    "\n",
    "loss(model) = (x,y)->logitcrossentropy(model(x), y)\n",
    "loss(model,x,y) = loss(model)(x, y)\n",
    "loss(xy, model) = loss(model)(hcat(map(first,xy)...), hcat(map(last,xy)...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a multi-layer perceptron (MLP) model for our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4, 15, relu),                   \u001b[90m# 75 parameters\u001b[39m\n",
       "  Dense(15, 3),                         \u001b[90m# 48 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 4 arrays, \u001b[39m123 parameters, 748 bytes."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(Dense(d, 15,relu), Dense(15, nclasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the backpropagation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify a fitness function. We have already defined a loss function for our backpropagation model, so we are going to reuse it.\n",
    "\n",
    "- We pass an individual to the fitness function to evaluate a loss of the MLP.\n",
    "- GA optimization searches an individual to minimize the fitness function. In our case optimization direction is aligned with the backpropagation model loss function as we seek to minimize the MLP loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitness (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(m) = loss(data, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3645393195604965"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Operators\n",
    "\n",
    "We need to define a crossover operator to combine the information of two parents model to generate new individuals. The objective is to increase genetic variability and provide better options.\n",
    "- Flattent the MLP networks into parameter vector representations\n",
    "- Perform a crossover operation on the parameter vectors\n",
    "- Reconstruct MLPs from the parameter vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniform_mlp (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function uniform_mlp(m1::T, m2::T) where {T <: Chain}\n",
    "    θ1, re1 = Flux.destructure(m1);\n",
    "    θ2, re2 = Flux.destructure(m2);\n",
    "    c1, c2 =uniform(θ1,θ2)\n",
    "    return re1(c1), re2(c2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Chain(Dense(4, 15, relu), Dense(15, 3)), Chain(Dense(4, 15, relu), Dense(15, 3)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_mlp(model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we rewrite a `gaussian` mutatation operator for MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gaussian_mlp (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gaussian_mlp(σ::Real = 1.0)\n",
    "    vop = gaussian(σ)\n",
    "    function mutation(recombinant::T) where {T <: Chain}        \n",
    "        θ, re = Flux.destructure(recombinant)\n",
    "        return re(convert(Vector{Float32}, vop(θ)))\n",
    "    end\n",
    "    return mutation\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(4, 15, relu),                   \u001b[90m# 75 parameters\u001b[39m\n",
       "  Dense(15, 3),                         \u001b[90m# 48 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 4 arrays, \u001b[39m123 parameters, 748 bytes."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_mlp(0.5)(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population\n",
    "\n",
    "We also require a function for generating a random population of the individuls required for evolutionary optimizations. Our polulation consists of MLP objects, `Flux.Chain` type.\n",
    "\n",
    "We need to override `Evolutionary.initial_population` which will allows us to create population of the random MPL objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initial_population (generic function with 6 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Evolutionary.initial_population\n",
    "function initial_population(method::M, individual::Chain) where {M<:Evolutionary.AbstractOptimizer}\n",
    "    θ, re = Flux.destructure(individual);\n",
    "    [re(randn(length(θ))) for i in 1:Evolutionary.population_size(method)]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Evolutionary` optimization algorithms are designed work with a individual population represented as a collection of numerical vectors.\n",
    "- The optimization objective is kept as a `NonDifferentiable` object works with any numerical vector type. This object allows to keep a minimizer value, an objective function and its value for minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonDifferentiable{Float64, Vector{Float64}}(sum, 0.0, [NaN, NaN, NaN, NaN, NaN], [0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NonDifferentiable(sum, rand(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this type doesn't have a constructor for working with an MLP object, that is of `Flux.Chain` type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this missing constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonDifferentiable"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Evolutionary.NonDifferentiable\n",
    "NonDifferentiable(f, x::Chain) = NonDifferentiable{Real,typeof(x)}(f, f(x), deepcopy(x), [0,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly, we are required to make a copy of individuals, but `Flux` doesn't provide a `copy` & `copyto!` functions for `Chain` objects, only `deepcopy`. We are going to define some missing functions.\n",
    "- Test if your individual object successully makes a copy using `copy` and `copyto!` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "copyto! (generic function with 134 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: copy, copyto!\n",
    "\n",
    "copy(ch::Chain) = deepcopy(ch)\n",
    "\n",
    "function copyto!(l1::Dense{T}, l2::Dense{T}) where {T}\n",
    "    copyto!(l1.W, l2.W)\n",
    "    copyto!(l1.b, l2.b)\n",
    "    l1\n",
    "end\n",
    "\n",
    "function copyto!(ch1::Chain, ch2::Chain)\n",
    "    for i in 1:length(ch1.layers)\n",
    "        copyto!(ch1.layers[i],ch2.layers[i])\n",
    "    end\n",
    "    ch1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the parameters of our evolutionary optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  abstol = 1.0e-32\n",
       "                  reltol = 1.0e-32\n",
       "        successive_f_tol = 10\n",
       "              iterations = 10000\n",
       "             store_trace = false\n",
       "              show_trace = false\n",
       "              show_every = 1\n",
       "                callback = nothing\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts = Evolutionary.Options(iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GA(100, 0.95, 0.95, 0.002, Evolutionary.rouletteinv, uniform_mlp, var\"#mutation#7\"{Evolutionary.var\"#mutation#29\"{Float64}}(Evolutionary.var\"#mutation#29\"{Float64}(2.0)))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = GA(\n",
    "        selection = rouletteinv,\n",
    "        mutation =  gaussian_mlp(2.0),\n",
    "        crossover = uniform_mlp,\n",
    "        mutationRate = 0.95,\n",
    "        crossoverRate = 0.95,\n",
    "        populationSize = 100,\n",
    "        ε = 0.002\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching (::var\"#mutation#7\"{Evolutionary.var\"#mutation#29\"{Float64}})(::Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}, ::NoStrategy)\n\u001b[0mClosest candidates are:\n\u001b[0m  (::var\"#mutation#7\")(::T) where T<:Chain at In[10]:3",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::var\"#mutation#7\"{Evolutionary.var\"#mutation#29\"{Float64}})(::Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}, ::NoStrategy)\n\u001b[0mClosest candidates are:\n\u001b[0m  (::var\"#mutation#7\")(::T) where T<:Chain at In[10]:3",
      "",
      "Stacktrace:",
      " [1] update_state!(objfun::NonDifferentiable{Real, Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}}, constraints::Evolutionary.NoConstraints, state::Evolutionary.ESState{Float64, Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}, AbstractStrategy}, population::Vector{Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}}, method::ES, itr::Int64)",
      "   @ Evolutionary C:\\Users\\xkzmx\\.julia\\packages\\Evolutionary\\BIk3j\\src\\es.jl:101",
      " [2] optimize(objfun::NonDifferentiable{Real, Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}}, constraints::Evolutionary.NoConstraints, population::Vector{Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}}, method::ES, options::Evolutionary.Options{Nothing}, state::Evolutionary.ESState{Float64, Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}, AbstractStrategy})",
      "   @ Evolutionary C:\\Users\\xkzmx\\.julia\\packages\\Evolutionary\\BIk3j\\src\\api\\optimize.jl:56",
      " [3] optimize(objfun::NonDifferentiable{Real, Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}}, constraints::Evolutionary.NoConstraints, population::Vector{Chain{Tuple{Dense{typeof(relu), Matrix{Float64}, Vector{Float64}}, Dense{typeof(identity), Matrix{Float64}, Vector{Float64}}}}}, method::ES, options::Evolutionary.Options{Nothing})",
      "   @ Evolutionary C:\\Users\\xkzmx\\.julia\\packages\\Evolutionary\\BIk3j\\src\\api\\optimize.jl:40",
      " [4] optimize(f::Function, constraints::Evolutionary.NoConstraints, individual::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, method::ES, options::Evolutionary.Options{Nothing})",
      "   @ Evolutionary C:\\Users\\xkzmx\\.julia\\packages\\Evolutionary\\BIk3j\\src\\api\\optimize.jl:31",
      " [5] optimize(f::Function, individual::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, method::ES, options::Evolutionary.Options{Nothing})",
      "   @ Evolutionary C:\\Users\\xkzmx\\.julia\\packages\\Evolutionary\\BIk3j\\src\\api\\optimize.jl:13",
      " [6] top-level scope",
      "   @ In[44]:2",
      " [7] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "Random.seed!(2947);\n",
    "res = Evolutionary.optimize(fitness, model, ES(mutation =gaussian_mlp(2.0),mu = 10,lambda = 70), opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MLP\n",
      "│   loss = 3.4680285804291158\n",
      "│   accuracy = 0.38\n",
      "└ @ Main In[39]:2\n"
     ]
    }
   ],
   "source": [
    "evomodel= Evolutionary.minimizer(res)\n",
    "@info \"MLP\" loss=loss(data, evomodel) accuracy = accuracy(data, evomodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

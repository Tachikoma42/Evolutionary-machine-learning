{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Evolutionary\n",
    "using Flux\n",
    "using Flux: onehot, onecold, logitcrossentropy #, throttle, @epochs\n",
    "using MLDatasets\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using Iris dataset for this exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 150)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = Iris.features();\n",
    "slabels = Iris.labels();\n",
    "classes = unique(slabels)  # unique classes in the dataset\n",
    "nclasses = length(classes) # number of classes\n",
    "d, n = size(features)          # dimension and size if the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert feature and labels in appropriate for training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Array{Tuple{SubArray{Float64,1,Array{Float64,2},Tuple{Base.Slice{Base.OneTo{Int64}},Int64},true},Flux.OneHotVector},1}:\n",
       " ([5.1, 3.5, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.9, 3.0, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.7, 3.2, 1.3, 0.2], [1, 0, 0])\n",
       " ([4.6, 3.1, 1.5, 0.2], [1, 0, 0])\n",
       " ([5.0, 3.6, 1.4, 0.2], [1, 0, 0])\n",
       " ([5.4, 3.9, 1.7, 0.4], [1, 0, 0])\n",
       " ([4.6, 3.4, 1.4, 0.3], [1, 0, 0])\n",
       " ([5.0, 3.4, 1.5, 0.2], [1, 0, 0])\n",
       " ([4.4, 2.9, 1.4, 0.2], [1, 0, 0])\n",
       " ([4.9, 3.1, 1.5, 0.1], [1, 0, 0])\n",
       " ([5.4, 3.7, 1.5, 0.2], [1, 0, 0])\n",
       " ([4.8, 3.4, 1.6, 0.2], [1, 0, 0])\n",
       " ([4.8, 3.0, 1.4, 0.1], [1, 0, 0])\n",
       " ⋮                                \n",
       " ([6.0, 3.0, 4.8, 1.8], [0, 0, 1])\n",
       " ([6.9, 3.1, 5.4, 2.1], [0, 0, 1])\n",
       " ([6.7, 3.1, 5.6, 2.4], [0, 0, 1])\n",
       " ([6.9, 3.1, 5.1, 2.3], [0, 0, 1])\n",
       " ([5.8, 2.7, 5.1, 1.9], [0, 0, 1])\n",
       " ([6.8, 3.2, 5.9, 2.3], [0, 0, 1])\n",
       " ([6.7, 3.3, 5.7, 2.5], [0, 0, 1])\n",
       " ([6.7, 3.0, 5.2, 2.3], [0, 0, 1])\n",
       " ([6.3, 2.5, 5.0, 1.9], [0, 0, 1])\n",
       " ([6.5, 3.0, 5.2, 2.0], [0, 0, 1])\n",
       " ([6.2, 3.4, 5.4, 2.3], [0, 0, 1])\n",
       " ([5.9, 3.0, 5.1, 1.8], [0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ (x, onehot(l, classes)) for (x, l) in zip(eachcol(features), slabels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some auxiliary functions: model accuracy and its loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 3 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,x,y) = sum(onecold(model(x)) .== onecold(y))/size(x,2)\n",
    "accuracy(xy, model) = sum( onecold(model(x)) .== onecold(y) for (x,y) in xy) /length(xy)\n",
    "\n",
    "loss(model) = (x,y)->logitcrossentropy(model(x), y)\n",
    "loss(model,x,y) = loss(model)(x, y)\n",
    "loss(xy, model) = loss(model)(hcat(map(first,xy)...), hcat(map(last,xy)...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a multi-layer perceptron (MLP) model for our classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(4, 15, relu), Dense(15, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(Dense(d, 15, relu), Dense(15, nclasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using the backpropagation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(data, model) = 1.2993094f0\n",
      "accuracy(data, model) = 0.013333333333333334\n",
      "loss(data, model) = 0.22241203f0\n",
      "accuracy(data, model) = 0.9733333333333334\n",
      "loss(data, model) = 0.12049951f0\n",
      "accuracy(data, model) = 0.98\n"
     ]
    }
   ],
   "source": [
    "opt = ADAM(1e-4)\n",
    "evalcb = Flux.throttle(() -> @show(loss(data, model), accuracy(data, model)), 5)\n",
    "for i in 1:500\n",
    "    Flux.train!(loss(model), params(model), data, opt, cb = evalcb)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MLP\n",
      "│   loss = 0.088482074\n",
      "│   accuracy = 0.9733333333333334\n",
      "└ @ Main In[13]:1\n"
     ]
    }
   ],
   "source": [
    "@info \"MLP\" loss=loss(data, model) accuracy = accuracy(data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify a fitness function. We have already defined a loss function for our backpropagation model, so we are going to reuse it.\n",
    "\n",
    "- We pass an individual to the fitness function to evaluate a loss of the MLP.\n",
    "- GA optimization searches an individual to minimize the fitness function. In our case optimization direction is aligned with the backpropagation model loss function as we seek to minimize the MLP loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitness (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(m) = loss(data, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.088482074f0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Operators\n",
    "\n",
    "We need to define a crossover operator to combine the information of two parents model to generate new individuals. The objective is to increase genetic variability and provide better options.\n",
    "- Flattent the MLP networks into parameter vector representations\n",
    "- Perform a crossover operation on the parameter vectors\n",
    "- Reconstruct MLPs from the parameter vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: Chain not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Chain not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:1",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "function uniform_mlp(m1::T, m2::T) where {T <: Chain}\n",
    "    θ1, re1 = Flux.destructure(m1);\n",
    "    θ2, re2 = Flux.destructure(m2);\n",
    "    c1, c2 =uniform(θ1,θ2)\n",
    "    return re1(c1), re2(c2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Chain(Dense(4, 15, relu), Dense(15, 3)), Chain(Dense(4, 15, relu), Dense(15, 3)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_mlp(model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we rewrite a `gaussian` mutatation operator for MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: Chain not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Chain not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:1",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "function gaussian_mlp(σ::Real = 1.0)\n",
    "    vop = gaussian(σ)\n",
    "    function mutation(recombinant::T) where {T <: Chain}        \n",
    "        θ, re = Flux.destructure(recombinant)\n",
    "        return re(convert(Vector{Float32}, vop(θ)))\n",
    "    end\n",
    "    return mutation\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(4, 15, relu), Dense(15, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_mlp(0.5)(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population\n",
    "\n",
    "We also require a function for generating a random population of the individuls required for evolutionary optimizations. Our polulation consists of MLP objects, `Flux.Chain` type.\n",
    "\n",
    "We need to override `Evolutionary.initial_population` which will allows us to create population of the random MPL objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initial_population (generic function with 6 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Evolutionary.initial_population\n",
    "function initial_population(method::M, individual::Chain) where {M<:Evolutionary.AbstractOptimizer}\n",
    "    θ, re = Flux.destructure(individual);\n",
    "    [re(randn(length(θ))) for i in 1:Evolutionary.population_size(method)]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Evolutionary` optimization algorithms are designed work with a individual population represented as a collection of numerical vectors.\n",
    "- The optimization objective is kept as a `NonDifferentiable` object works with any numerical vector type. This object allows to keep a minimizer value, an objective function and its value for minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonDifferentiable{Float64,Array{Float64,1}}(sum, 0.0, [NaN, NaN, NaN, NaN, NaN], [0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NonDifferentiable(sum, rand(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this type doesn't have a constructor for working with an MLP object, that is of `Flux.Chain` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching NonDifferentiable(::typeof(fitness), ::Chain{Tuple{Dense{typeof(relu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}})\nClosest candidates are:\n  NonDifferentiable(::Any, ::Any, !Matched::AbstractArray) at /home/art/.julia/packages/NLSolversBase/QPnui/src/objective_types/nondifferentiable.jl:21\n  NonDifferentiable(::Any, ::TF, !Matched::TX, !Matched::Array{Int64,1}) where {TF, TX} at /home/art/.julia/packages/NLSolversBase/QPnui/src/objective_types/nondifferentiable.jl:3\n  NonDifferentiable(::Any, ::Any, !Matched::AbstractArray, !Matched::Union{Real, AbstractArray}) at /home/art/.julia/packages/NLSolversBase/QPnui/src/objective_types/nondifferentiable.jl:21\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching NonDifferentiable(::typeof(fitness), ::Chain{Tuple{Dense{typeof(relu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}})\nClosest candidates are:\n  NonDifferentiable(::Any, ::Any, !Matched::AbstractArray) at /home/art/.julia/packages/NLSolversBase/QPnui/src/objective_types/nondifferentiable.jl:21\n  NonDifferentiable(::Any, ::TF, !Matched::TX, !Matched::Array{Int64,1}) where {TF, TX} at /home/art/.julia/packages/NLSolversBase/QPnui/src/objective_types/nondifferentiable.jl:3\n  NonDifferentiable(::Any, ::Any, !Matched::AbstractArray, !Matched::Union{Real, AbstractArray}) at /home/art/.julia/packages/NLSolversBase/QPnui/src/objective_types/nondifferentiable.jl:21\n  ...",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[22]:1"
     ]
    }
   ],
   "source": [
    "NonDifferentiable(fitness, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this missing constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonDifferentiable"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Evolutionary.NonDifferentiable\n",
    "NonDifferentiable(f, x::Chain) = NonDifferentiable{Real,typeof(x)}(f, f(x), deepcopy(x), [0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonDifferentiable{Real,Chain{Tuple{Dense{typeof(relu),Array{Float32,2},Array{Float32,1}},Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}}}(fitness, 0.088482074f0, Chain(Dense(4, 15, relu), Dense(15, 3)), [0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NonDifferentiable(fitness, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly, we are required to make a copy of individuals, but `Flux` doesn't provide a `copy` & `copyto!` functions for `Chain` objects, only `deepcopy`. We are going to define some missing functions.\n",
    "- Test if your individual object successully makes a copy using `copy` and `copyto!` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "copyto! (generic function with 131 methods)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: copy, copyto!\n",
    "\n",
    "copy(ch::Chain) = deepcopy(ch)\n",
    "\n",
    "function copyto!(l1::Dense{T}, l2::Dense{T}) where {T}\n",
    "    copyto!(l1.W, l2.W)\n",
    "    copyto!(l1.b, l2.b)\n",
    "    l1\n",
    "end\n",
    "\n",
    "function copyto!(ch1::Chain, ch2::Chain)\n",
    "    for i in 1:length(ch1.layers)\n",
    "        copyto!(ch1.layers[i],ch2.layers[i])\n",
    "    end\n",
    "    ch1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the parameters of our evolutionary optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  abstol = 1.0e-32\n",
       "                  reltol = 1.0e-32\n",
       "        successive_f_tol = 10\n",
       "              iterations = 1000\n",
       "             store_trace = true\n",
       "              show_trace = false\n",
       "              show_every = 3\n",
       "                callback = nothing\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts = Evolutionary.Options(iterations=1000, show_every=3, show_trace=false, store_trace=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GA(100, 0.95, 0.95, 0.02, Evolutionary.rouletteinv, uniform_mlp, var\"#mutation#19\"{Evolutionary.var\"#mutation#27\"{Float64}}(Evolutionary.var\"#mutation#27\"{Float64}(2.0)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = GA(\n",
    "        selection = rouletteinv,\n",
    "        mutation =  gaussian_mlp(2.0),\n",
    "        crossover = uniform_mlp,\n",
    "        mutationRate = 0.95,\n",
    "        crossoverRate = 0.95,\n",
    "        populationSize = 100,\n",
    "        ε = 0.02\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer:  [Dense(4, 15, relu), Dense(15, 3)]\n",
       "    Minimum:    0.08224526216169942\n",
       "    Iterations: 69\n",
       "\n",
       " * Found with\n",
       "    Algorithm: GA[P=100,x=0.95,μ=0.95,ɛ=0.02]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(2947);\n",
    "res = Evolutionary.optimize(fitness, model, algo, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MLP\n",
      "│   loss = 0.08224526216169942\n",
      "│   accuracy = 0.9733333333333334\n",
      "└ @ Main In[32]:2\n"
     ]
    }
   ],
   "source": [
    "evomodel= Evolutionary.minimizer(res)\n",
    "@info \"MLP\" loss=loss(data, evomodel) accuracy = accuracy(data, evomodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: onehot, onecold, logitcrossentropy #, throttle, @epochs\n",
    "using MLDatasets\n",
    "using Random\n",
    "using JLD2\n",
    "using Statistics\n",
    "using MLJBase\n",
    "using BSON: @load # for load weights\n",
    "using Plots\n",
    "using Zygote\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = JLD2.load_object(\"data/train_data.jld2\");\n",
    "test_data = JLD2.load_object(\"data/test_data.jld2\");\n",
    "x_train = JLD2.load_object(\"data/x_train.jld2\");\n",
    "y_train = JLD2.load_object(\"data/y_train.jld2\");\n",
    "x_test  = JLD2.load_object(\"data/x_test.jld2\");\n",
    "y_test  = JLD2.load_object(\"data/y_test.jld2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(19, 38, σ),                     \u001b[90m# 760 parameters\u001b[39m\n",
       "  Dense(38, 2),                         \u001b[90m# 78 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 4 arrays, \u001b[39m838 parameters, 3.523 KiB."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "N = 50\n",
    "w0 = 0.8\n",
    "c1 = 1.6\n",
    "c2 = 1.8\n",
    "\n",
    "rng =Random.default_rng()\n",
    "model = Chain(Dense(19, 2*19, sigmoid), Dense(2*19, 2))#define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 3 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model,x,y) = sum(onecold(model(x)) .== onecold(y))/size(x,2)\n",
    "accuracy(xy, model) = mean( onecold(model(x)) .== onecold(y) for (x,y) in xy)\n",
    "loss(model) = (x,y)->Flux.logitcrossentropy(model(x), y)\n",
    "loss(model,x,y) = loss(model)(x, y)\n",
    "loss(xy, model) = loss(model)(hcat(map(first,xy)...), hcat(map(last,xy)...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSO training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: MLP\n",
      "│   loss = 0.6834331164725335\n",
      "│   accuracy = 0.5612391930835735\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6834331164725335\n",
      "│   accuracy = 0.5612391930835735\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6834331164725335\n",
      "│   accuracy = 0.5612391930835735\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6834331164725335\n",
      "│   accuracy = 0.5612391930835735\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6834331164725335\n",
      "│   accuracy = 0.5612391930835735\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6851349038344289\n",
      "│   accuracy = 0.553314121037464\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6851349038344289\n",
      "│   accuracy = 0.553314121037464\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6851349038344289\n",
      "│   accuracy = 0.553314121037464\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6851349038344289\n",
      "│   accuracy = 0.553314121037464\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6851349038344289\n",
      "│   accuracy = 0.553314121037464\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6574371768725796\n",
      "│   accuracy = 0.611671469740634\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6574371768725796\n",
      "│   accuracy = 0.611671469740634\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6574371768725796\n",
      "│   accuracy = 0.611671469740634\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6574371768725796\n",
      "│   accuracy = 0.611671469740634\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6574371768725796\n",
      "│   accuracy = 0.611671469740634\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.677007221382182\n",
      "│   accuracy = 0.6030259365994236\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727543011795506\n",
      "│   accuracy = 0.6023054755043228\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727543011795506\n",
      "│   accuracy = 0.6023054755043228\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727543011795506\n",
      "│   accuracy = 0.6023054755043228\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727543011795506\n",
      "│   accuracy = 0.6023054755043228\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6707134478490514\n",
      "│   accuracy = 0.579971181556196\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6707134478490514\n",
      "│   accuracy = 0.579971181556196\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6707134478490514\n",
      "│   accuracy = 0.579971181556196\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6707134478490514\n",
      "│   accuracy = 0.579971181556196\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6707134478490514\n",
      "│   accuracy = 0.579971181556196\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788547279951666\n",
      "│   accuracy = 0.5345821325648416\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788547279951666\n",
      "│   accuracy = 0.5345821325648416\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788547279951666\n",
      "│   accuracy = 0.5345821325648416\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788547279951666\n",
      "│   accuracy = 0.5345821325648416\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788547279951666\n",
      "│   accuracy = 0.5345821325648416\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6768449933585824\n",
      "│   accuracy = 0.5828530259365994\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6715756754027631\n",
      "│   accuracy = 0.5871757925072046\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6715756754027631\n",
      "│   accuracy = 0.5871757925072046\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6715756754027631\n",
      "│   accuracy = 0.5871757925072046\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6715756754027631\n",
      "│   accuracy = 0.5871757925072046\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6651672439558673\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6651672439558673\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6651672439558673\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6651672439558673\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6651672439558673\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6827624186552249\n",
      "│   accuracy = 0.5540345821325648\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727204502574189\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727204502574189\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727204502574189\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6727204502574189\n",
      "│   accuracy = 0.6260806916426513\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6854655072663043\n",
      "│   accuracy = 0.5756484149855908\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6805531089552419\n",
      "│   accuracy = 0.5979827089337176\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788408458438139\n",
      "│   accuracy = 0.5972622478386167\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788408458438139\n",
      "│   accuracy = 0.5972622478386167\n",
      "└ @ Main In[5]:37\n",
      "┌ Info: MLP\n",
      "│   loss = 0.6788408458438139\n",
      "│   accuracy = 0.5972622478386167\n",
      "└ @ Main In[5]:37\n"
     ]
    }
   ],
   "source": [
    "# change population_size will increase training time, but may increase the accuracy\n",
    "save_resultTest = zeros(epochs);\n",
    "save_resultTrain = zeros(epochs);\n",
    "# l1 = Dense(19, 2*19, sigmoid)\n",
    "# l2 = Dense(2*19, 2)\n",
    "for k in 1:10\n",
    "    loadpath = string(\"models/model\",k,\".bson\")\n",
    "    @load loadpath weights\n",
    "    Flux.loadparams!(model, weights)\n",
    "    θ, re = Flux.destructure(model)\n",
    "    weights = [Vector{Float64}(undef,size(θ)[1]) for _ in 1:N];\n",
    "    vec = [Vector{Float64}(undef,size(θ)[1]) for _ in 1:N];\n",
    "\n",
    "    for i in 1:N\n",
    "        vec[i] = rand(size(θ)[1])\n",
    "        for j in 1:size(θ)[1]\n",
    "            weights[i][j] = rand(Normal(θ[i], 0.05))\n",
    "        end\n",
    "\n",
    "    end\n",
    "    p_best = copy(weights);\n",
    "    g_best = sort(weights, lt=(x,y)->isless(loss(train_data, re(x)), loss(train_data, re(y))))[1];\n",
    "\n",
    "    for i in 1:epochs\n",
    "        for j in 1:N\n",
    "            vec[j] = w0 *vec[j] + c1 *(1.0 - rand()) * (p_best[j] - vec[j]) + c2 * (1.0 - rand()) * (g_best - vec[j])\n",
    "            weights[j] = vec[j]+weights[j]\n",
    "            if loss(train_data, re(weights[j])) < loss(train_data, re(p_best[j]))\n",
    "                p_best[j] = weights[j]\n",
    "            end\n",
    "        end\n",
    "        g_best = sort(p_best, lt=(x,y)->isless(loss(train_data, re(x)), loss(train_data, re(y))))[1];\n",
    "\n",
    "        save_resultTest[i] = save_resultTest[i] + accuracy(test_data, re(g_best))\n",
    "        save_resultTrain[i] = save_resultTrain[i] + accuracy(train_data, re(g_best))\n",
    "        if i%(epochs/5) == 0\n",
    "            @info \"MLP\"  loss=loss(train_data, re(g_best)) accuracy = accuracy(train_data, re(g_best))\n",
    "        end\n",
    "        \n",
    "\n",
    "    end\n",
    "    \n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot&Print outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip510\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip510)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip511\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip510)\" d=\"\nM708.588 1486.45 L2352.76 1486.45 L2352.76 123.472 L708.588 123.472  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip512\">\n    <rect x=\"708\" y=\"123\" width=\"1645\" height=\"1364\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  755.122,1486.45 755.122,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1272.89,1486.45 1272.89,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1790.66,1486.45 1790.66,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2308.43,1486.45 2308.43,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  708.588,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  755.122,1486.45 755.122,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1272.89,1486.45 1272.89,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1790.66,1486.45 1790.66,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2308.43,1486.45 2308.43,1467.55 \n  \"/>\n<path clip-path=\"url(#clip510)\" d=\"M755.122 1517.37 Q751.51 1517.37 749.682 1520.93 Q747.876 1524.47 747.876 1531.6 Q747.876 1538.71 749.682 1542.27 Q751.51 1545.82 755.122 1545.82 Q758.756 1545.82 760.561 1542.27 Q762.39 1538.71 762.39 1531.6 Q762.39 1524.47 760.561 1520.93 Q758.756 1517.37 755.122 1517.37 M755.122 1513.66 Q760.932 1513.66 763.987 1518.27 Q767.066 1522.85 767.066 1531.6 Q767.066 1540.33 763.987 1544.94 Q760.932 1549.52 755.122 1549.52 Q749.311 1549.52 746.233 1544.94 Q743.177 1540.33 743.177 1531.6 Q743.177 1522.85 746.233 1518.27 Q749.311 1513.66 755.122 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1263.27 1544.91 L1270.91 1544.91 L1270.91 1518.55 L1262.6 1520.21 L1262.6 1515.95 L1270.87 1514.29 L1275.54 1514.29 L1275.54 1544.91 L1283.18 1544.91 L1283.18 1548.85 L1263.27 1548.85 L1263.27 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1785.32 1544.91 L1801.63 1544.91 L1801.63 1548.85 L1779.69 1548.85 L1779.69 1544.91 Q1782.35 1542.16 1786.94 1537.53 Q1791.54 1532.88 1792.72 1531.53 Q1794.97 1529.01 1795.85 1527.27 Q1796.75 1525.51 1796.75 1523.82 Q1796.75 1521.07 1794.81 1519.33 Q1792.88 1517.6 1789.78 1517.6 Q1787.58 1517.6 1785.13 1518.36 Q1782.7 1519.13 1779.92 1520.68 L1779.92 1515.95 Q1782.75 1514.82 1785.2 1514.24 Q1787.65 1513.66 1789.69 1513.66 Q1795.06 1513.66 1798.25 1516.35 Q1801.45 1519.03 1801.45 1523.52 Q1801.45 1525.65 1800.64 1527.57 Q1799.85 1529.47 1797.75 1532.07 Q1797.17 1532.74 1794.06 1535.95 Q1790.96 1539.15 1785.32 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M2312.68 1530.21 Q2316.04 1530.93 2317.91 1533.2 Q2319.81 1535.47 2319.81 1538.8 Q2319.81 1543.92 2316.29 1546.72 Q2312.77 1549.52 2306.29 1549.52 Q2304.12 1549.52 2301.8 1549.08 Q2299.51 1548.66 2297.06 1547.81 L2297.06 1543.29 Q2299 1544.43 2301.31 1545.01 Q2303.63 1545.58 2306.15 1545.58 Q2310.55 1545.58 2312.84 1543.85 Q2315.16 1542.11 2315.16 1538.8 Q2315.16 1535.75 2313 1534.03 Q2310.87 1532.3 2307.06 1532.3 L2303.03 1532.3 L2303.03 1528.45 L2307.24 1528.45 Q2310.69 1528.45 2312.52 1527.09 Q2314.35 1525.7 2314.35 1523.11 Q2314.35 1520.45 2312.45 1519.03 Q2310.57 1517.6 2307.06 1517.6 Q2305.13 1517.6 2302.93 1518.01 Q2300.74 1518.43 2298.1 1519.31 L2298.1 1515.14 Q2300.76 1514.4 2303.07 1514.03 Q2305.41 1513.66 2307.47 1513.66 Q2312.8 1513.66 2315.9 1516.09 Q2319 1518.5 2319 1522.62 Q2319 1525.49 2317.36 1527.48 Q2315.71 1529.45 2312.68 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  708.588,1298.75 2352.76,1298.75 \n  \"/>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  708.588,964.624 2352.76,964.624 \n  \"/>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  708.588,630.497 2352.76,630.497 \n  \"/>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  708.588,296.371 2352.76,296.371 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  708.588,1486.45 708.588,123.472 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  708.588,1298.75 722.933,1298.75 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  708.588,964.624 722.933,964.624 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  708.588,630.497 722.933,630.497 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  708.588,296.371 722.933,296.371 \n  \"/>\n<path clip-path=\"url(#clip510)\" d=\"M586.848 1284.55 Q583.237 1284.55 581.408 1288.11 Q579.603 1291.65 579.603 1298.78 Q579.603 1305.89 581.408 1309.46 Q583.237 1313 586.848 1313 Q590.482 1313 592.288 1309.46 Q594.117 1305.89 594.117 1298.78 Q594.117 1291.65 592.288 1288.11 Q590.482 1284.55 586.848 1284.55 M586.848 1280.84 Q592.658 1280.84 595.714 1285.45 Q598.793 1290.03 598.793 1298.78 Q598.793 1307.51 595.714 1312.12 Q592.658 1316.7 586.848 1316.7 Q581.038 1316.7 577.959 1312.12 Q574.904 1307.51 574.904 1298.78 Q574.904 1290.03 577.959 1285.45 Q581.038 1280.84 586.848 1280.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M607.01 1310.15 L611.894 1310.15 L611.894 1316.03 L607.01 1316.03 L607.01 1310.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M622.126 1281.47 L640.482 1281.47 L640.482 1285.4 L626.408 1285.4 L626.408 1293.88 Q627.427 1293.53 628.445 1293.37 Q629.464 1293.18 630.482 1293.18 Q636.269 1293.18 639.649 1296.35 Q643.028 1299.53 643.028 1304.94 Q643.028 1310.52 639.556 1313.62 Q636.084 1316.7 629.765 1316.7 Q627.589 1316.7 625.32 1316.33 Q623.075 1315.96 620.667 1315.22 L620.667 1310.52 Q622.751 1311.65 624.973 1312.21 Q627.195 1312.77 629.672 1312.77 Q633.677 1312.77 636.015 1310.66 Q638.353 1308.55 638.353 1304.94 Q638.353 1301.33 636.015 1299.22 Q633.677 1297.12 629.672 1297.12 Q627.797 1297.12 625.922 1297.53 Q624.07 1297.95 622.126 1298.83 L622.126 1281.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M656.269 1312.09 L672.588 1312.09 L672.588 1316.03 L650.644 1316.03 L650.644 1312.09 Q653.306 1309.34 657.889 1304.71 Q662.496 1300.06 663.676 1298.71 Q665.922 1296.19 666.801 1294.46 Q667.704 1292.7 667.704 1291.01 Q667.704 1288.25 665.76 1286.52 Q663.839 1284.78 660.737 1284.78 Q658.538 1284.78 656.084 1285.54 Q653.653 1286.31 650.876 1287.86 L650.876 1283.14 Q653.7 1282 656.153 1281.42 Q658.607 1280.84 660.644 1280.84 Q666.014 1280.84 669.209 1283.53 Q672.403 1286.22 672.403 1290.71 Q672.403 1292.84 671.593 1294.76 Q670.806 1296.65 668.7 1299.25 Q668.121 1299.92 665.019 1303.14 Q661.917 1306.33 656.269 1312.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M584.765 950.422 Q581.154 950.422 579.325 953.987 Q577.52 957.529 577.52 964.658 Q577.52 971.765 579.325 975.33 Q581.154 978.871 584.765 978.871 Q588.399 978.871 590.205 975.33 Q592.033 971.765 592.033 964.658 Q592.033 957.529 590.205 953.987 Q588.399 950.422 584.765 950.422 M584.765 946.719 Q590.575 946.719 593.631 951.325 Q596.709 955.908 596.709 964.658 Q596.709 973.385 593.631 977.992 Q590.575 982.575 584.765 982.575 Q578.955 982.575 575.876 977.992 Q572.82 973.385 572.82 964.658 Q572.82 955.908 575.876 951.325 Q578.955 946.719 584.765 946.719 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M604.927 976.024 L609.811 976.024 L609.811 981.904 L604.927 981.904 L604.927 976.024 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M620.042 947.344 L638.399 947.344 L638.399 951.279 L624.325 951.279 L624.325 959.751 Q625.343 959.404 626.362 959.242 Q627.38 959.056 628.399 959.056 Q634.186 959.056 637.566 962.228 Q640.945 965.399 640.945 970.816 Q640.945 976.394 637.473 979.496 Q634.001 982.575 627.681 982.575 Q625.505 982.575 623.237 982.205 Q620.992 981.834 618.584 981.093 L618.584 976.394 Q620.667 977.529 622.89 978.084 Q625.112 978.64 627.589 978.64 Q631.593 978.64 633.931 976.533 Q636.269 974.427 636.269 970.816 Q636.269 967.205 633.931 965.098 Q631.593 962.992 627.589 962.992 Q625.714 962.992 623.839 963.408 Q621.987 963.825 620.042 964.705 L620.042 947.344 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M663.005 951.418 L651.2 969.867 L663.005 969.867 L663.005 951.418 M661.778 947.344 L667.658 947.344 L667.658 969.867 L672.588 969.867 L672.588 973.755 L667.658 973.755 L667.658 981.904 L663.005 981.904 L663.005 973.755 L647.403 973.755 L647.403 969.242 L661.778 947.344 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M585.089 616.296 Q581.478 616.296 579.649 619.861 Q577.844 623.403 577.844 630.532 Q577.844 637.639 579.649 641.203 Q581.478 644.745 585.089 644.745 Q588.723 644.745 590.529 641.203 Q592.357 637.639 592.357 630.532 Q592.357 623.403 590.529 619.861 Q588.723 616.296 585.089 616.296 M585.089 612.592 Q590.899 612.592 593.955 617.199 Q597.033 621.782 597.033 630.532 Q597.033 639.259 593.955 643.865 Q590.899 648.449 585.089 648.449 Q579.279 648.449 576.2 643.865 Q573.145 639.259 573.145 630.532 Q573.145 621.782 576.2 617.199 Q579.279 612.592 585.089 612.592 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M605.251 641.898 L610.135 641.898 L610.135 647.777 L605.251 647.777 L605.251 641.898 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M620.367 613.217 L638.723 613.217 L638.723 617.153 L624.649 617.153 L624.649 625.625 Q625.667 625.278 626.686 625.116 Q627.704 624.93 628.723 624.93 Q634.51 624.93 637.89 628.102 Q641.269 631.273 641.269 636.69 Q641.269 642.268 637.797 645.37 Q634.325 648.449 628.005 648.449 Q625.829 648.449 623.561 648.078 Q621.316 647.708 618.908 646.967 L618.908 642.268 Q620.992 643.403 623.214 643.958 Q625.436 644.514 627.913 644.514 Q631.917 644.514 634.255 642.407 Q636.593 640.301 636.593 636.69 Q636.593 633.078 634.255 630.972 Q631.917 628.866 627.913 628.866 Q626.038 628.866 624.163 629.282 Q622.311 629.699 620.367 630.579 L620.367 613.217 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M661.061 628.634 Q657.913 628.634 656.061 630.787 Q654.232 632.94 654.232 636.69 Q654.232 640.416 656.061 642.592 Q657.913 644.745 661.061 644.745 Q664.209 644.745 666.038 642.592 Q667.889 640.416 667.889 636.69 Q667.889 632.94 666.038 630.787 Q664.209 628.634 661.061 628.634 M670.343 613.981 L670.343 618.241 Q668.584 617.407 666.778 616.967 Q664.996 616.528 663.237 616.528 Q658.607 616.528 656.153 619.653 Q653.723 622.778 653.376 629.097 Q654.741 627.083 656.802 626.018 Q658.862 624.93 661.339 624.93 Q666.547 624.93 669.556 628.102 Q672.588 631.25 672.588 636.69 Q672.588 642.014 669.44 645.231 Q666.292 648.449 661.061 648.449 Q655.065 648.449 651.894 643.865 Q648.723 639.259 648.723 630.532 Q648.723 622.338 652.612 617.477 Q656.501 612.592 663.051 612.592 Q664.811 612.592 666.593 612.94 Q668.399 613.287 670.343 613.981 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M585.344 282.17 Q581.732 282.17 579.904 285.735 Q578.098 289.277 578.098 296.406 Q578.098 303.513 579.904 307.077 Q581.732 310.619 585.344 310.619 Q588.978 310.619 590.783 307.077 Q592.612 303.513 592.612 296.406 Q592.612 289.277 590.783 285.735 Q588.978 282.17 585.344 282.17 M585.344 278.466 Q591.154 278.466 594.209 283.073 Q597.288 287.656 597.288 296.406 Q597.288 305.133 594.209 309.739 Q591.154 314.323 585.344 314.323 Q579.533 314.323 576.455 309.739 Q573.399 305.133 573.399 296.406 Q573.399 287.656 576.455 283.073 Q579.533 278.466 585.344 278.466 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M605.506 307.772 L610.39 307.772 L610.39 313.651 L605.506 313.651 L605.506 307.772 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M620.621 279.091 L638.978 279.091 L638.978 283.027 L624.904 283.027 L624.904 291.499 Q625.922 291.152 626.941 290.989 Q627.959 290.804 628.978 290.804 Q634.765 290.804 638.144 293.976 Q641.524 297.147 641.524 302.563 Q641.524 308.142 638.052 311.244 Q634.579 314.323 628.26 314.323 Q626.084 314.323 623.816 313.952 Q621.57 313.582 619.163 312.841 L619.163 308.142 Q621.246 309.276 623.468 309.832 Q625.691 310.388 628.167 310.388 Q632.172 310.388 634.51 308.281 Q636.848 306.175 636.848 302.563 Q636.848 298.952 634.51 296.846 Q632.172 294.739 628.167 294.739 Q626.292 294.739 624.417 295.156 Q622.566 295.573 620.621 296.452 L620.621 279.091 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M660.737 297.239 Q657.403 297.239 655.482 299.022 Q653.584 300.804 653.584 303.929 Q653.584 307.054 655.482 308.837 Q657.403 310.619 660.737 310.619 Q664.07 310.619 665.991 308.837 Q667.913 307.031 667.913 303.929 Q667.913 300.804 665.991 299.022 Q664.093 297.239 660.737 297.239 M656.061 295.249 Q653.052 294.508 651.362 292.448 Q649.695 290.388 649.695 287.425 Q649.695 283.281 652.635 280.874 Q655.598 278.466 660.737 278.466 Q665.899 278.466 668.838 280.874 Q671.778 283.281 671.778 287.425 Q671.778 290.388 670.088 292.448 Q668.422 294.508 665.436 295.249 Q668.815 296.036 670.69 298.327 Q672.588 300.619 672.588 303.929 Q672.588 308.952 669.51 311.638 Q666.454 314.323 660.737 314.323 Q655.019 314.323 651.94 311.638 Q648.885 308.952 648.885 303.929 Q648.885 300.619 650.783 298.327 Q652.681 296.036 656.061 295.249 M654.348 287.864 Q654.348 290.55 656.014 292.054 Q657.704 293.559 660.737 293.559 Q663.746 293.559 665.436 292.054 Q667.149 290.55 667.149 287.864 Q667.149 285.179 665.436 283.675 Q663.746 282.17 660.737 282.17 Q657.704 282.17 656.014 283.675 Q654.348 285.179 654.348 287.864 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1370.56 20.1573 L1359.46 50.2555 L1381.7 50.2555 L1370.56 20.1573 M1365.94 12.096 L1375.22 12.096 L1398.27 72.576 L1389.76 72.576 L1384.25 57.061 L1356.99 57.061 L1351.48 72.576 L1342.85 72.576 L1365.94 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1437.97 28.9478 L1437.97 35.9153 Q1434.81 34.1734 1431.61 33.3227 Q1428.45 32.4315 1425.21 32.4315 Q1417.96 32.4315 1413.95 37.0496 Q1409.93 41.6271 1409.93 49.9314 Q1409.93 58.2358 1413.95 62.8538 Q1417.96 67.4314 1425.21 67.4314 Q1428.45 67.4314 1431.61 66.5807 Q1434.81 65.6895 1437.97 63.9476 L1437.97 70.8341 Q1434.85 72.2924 1431.49 73.0216 Q1428.16 73.7508 1424.4 73.7508 Q1414.15 73.7508 1408.11 67.3098 Q1402.08 60.8689 1402.08 49.9314 Q1402.08 38.832 1408.15 32.472 Q1414.27 26.1121 1424.88 26.1121 Q1428.33 26.1121 1431.61 26.8413 Q1434.89 27.5299 1437.97 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1483.58 28.9478 L1483.58 35.9153 Q1480.42 34.1734 1477.22 33.3227 Q1474.06 32.4315 1470.82 32.4315 Q1463.57 32.4315 1459.56 37.0496 Q1455.55 41.6271 1455.55 49.9314 Q1455.55 58.2358 1459.56 62.8538 Q1463.57 67.4314 1470.82 67.4314 Q1474.06 67.4314 1477.22 66.5807 Q1480.42 65.6895 1483.58 63.9476 L1483.58 70.8341 Q1480.46 72.2924 1477.1 73.0216 Q1473.78 73.7508 1470.01 73.7508 Q1459.76 73.7508 1453.73 67.3098 Q1447.69 60.8689 1447.69 49.9314 Q1447.69 38.832 1453.77 32.472 Q1459.88 26.1121 1470.5 26.1121 Q1473.94 26.1121 1477.22 26.8413 Q1480.5 27.5299 1483.58 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1495.77 54.671 L1495.77 27.2059 L1503.23 27.2059 L1503.23 54.3874 Q1503.23 60.8284 1505.74 64.0691 Q1508.25 67.2693 1513.27 67.2693 Q1519.31 67.2693 1522.79 63.421 Q1526.32 59.5726 1526.32 52.9291 L1526.32 27.2059 L1533.77 27.2059 L1533.77 72.576 L1526.32 72.576 L1526.32 65.6084 Q1523.6 69.7404 1520 71.7658 Q1516.43 73.7508 1511.69 73.7508 Q1503.88 73.7508 1499.82 68.8897 Q1495.77 64.0286 1495.77 54.671 M1514.53 26.1121 L1514.53 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1575.41 34.1734 Q1574.16 33.4443 1572.66 33.1202 Q1571.2 32.7556 1569.42 32.7556 Q1563.1 32.7556 1559.7 36.8875 Q1556.33 40.9789 1556.33 48.6757 L1556.33 72.576 L1548.84 72.576 L1548.84 27.2059 L1556.33 27.2059 L1556.33 34.2544 Q1558.68 30.1225 1562.45 28.1376 Q1566.22 26.1121 1571.61 26.1121 Q1572.38 26.1121 1573.31 26.2337 Q1574.24 26.3147 1575.37 26.5172 L1575.41 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1603.85 49.7694 Q1594.82 49.7694 1591.33 51.8354 Q1587.85 53.9013 1587.85 58.8839 Q1587.85 62.8538 1590.44 65.2034 Q1593.08 67.5124 1597.57 67.5124 Q1603.77 67.5124 1607.5 63.1374 Q1611.26 58.7219 1611.26 51.4303 L1611.26 49.7694 L1603.85 49.7694 M1618.72 46.6907 L1618.72 72.576 L1611.26 72.576 L1611.26 65.6895 Q1608.71 69.8214 1604.91 71.8063 Q1601.1 73.7508 1595.59 73.7508 Q1588.62 73.7508 1584.49 69.8619 Q1580.4 65.9325 1580.4 59.3701 Q1580.4 51.7138 1585.5 47.825 Q1590.65 43.9361 1600.81 43.9361 L1611.26 43.9361 L1611.26 43.2069 Q1611.26 38.0623 1607.86 35.2672 Q1604.5 32.4315 1598.38 32.4315 Q1594.49 32.4315 1590.81 33.3632 Q1587.12 34.295 1583.72 36.1584 L1583.72 29.2718 Q1587.81 27.692 1591.66 26.9223 Q1595.51 26.1121 1599.15 26.1121 Q1609 26.1121 1613.86 31.2163 Q1618.72 36.3204 1618.72 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1666.72 28.9478 L1666.72 35.9153 Q1663.56 34.1734 1660.36 33.3227 Q1657.2 32.4315 1653.96 32.4315 Q1646.71 32.4315 1642.7 37.0496 Q1638.69 41.6271 1638.69 49.9314 Q1638.69 58.2358 1642.7 62.8538 Q1646.71 67.4314 1653.96 67.4314 Q1657.2 67.4314 1660.36 66.5807 Q1663.56 65.6895 1666.72 63.9476 L1666.72 70.8341 Q1663.6 72.2924 1660.24 73.0216 Q1656.92 73.7508 1653.15 73.7508 Q1642.9 73.7508 1636.87 67.3098 Q1630.83 60.8689 1630.83 49.9314 Q1630.83 38.832 1636.91 32.472 Q1643.02 26.1121 1653.64 26.1121 Q1657.08 26.1121 1660.36 26.8413 Q1663.64 27.5299 1666.72 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M1698.56 76.7889 Q1695.4 84.8907 1692.4 87.3618 Q1689.41 89.8329 1684.38 89.8329 L1678.43 89.8329 L1678.43 83.5945 L1682.8 83.5945 Q1685.88 83.5945 1687.58 82.1361 Q1689.29 80.6778 1691.35 75.2496 L1692.69 71.8468 L1674.34 27.2059 L1682.24 27.2059 L1696.41 62.6918 L1710.59 27.2059 L1718.49 27.2059 L1698.56 76.7889 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip512)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  755.122,1447.87 1114.01,892.865 1323.95,797.561 1472.9,831.197 1588.44,831.197 1682.84,609.755 1762.66,536.875 1831.79,497.632 1892.78,520.056 1947.33,520.056 \n  1996.68,520.056 2041.73,520.056 2083.18,520.056 2121.55,520.056 2157.27,520.056 2190.69,520.056 2222.08,520.056 2251.67,520.056 2279.66,520.056 2306.22,520.056 \n  \n  \"/>\n<polyline clip-path=\"url(#clip512)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  755.122,672.384 1114.01,357.034 1323.95,395.55 1472.9,325.74 1588.44,325.74 1682.84,258.337 1762.66,169.269 1831.79,162.047 1892.78,163.251 1947.33,163.251 \n  1996.68,163.251 2041.73,163.251 2083.18,163.251 2121.55,163.251 2157.27,163.251 2190.69,163.251 2222.08,163.251 2251.67,163.251 2279.66,163.251 2306.22,163.251 \n  \n  \"/>\n<path clip-path=\"url(#clip510)\" d=\"\nM114.56 324.425 L464.421 324.425 L464.421 168.905 L114.56 168.905  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  114.56,324.425 464.421,324.425 464.421,168.905 114.56,168.905 114.56,324.425 \n  \"/>\n<polyline clip-path=\"url(#clip510)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  138.56,220.745 282.56,220.745 \n  \"/>\n<path clip-path=\"url(#clip510)\" d=\"M306.56 203.465 L335.796 203.465 L335.796 207.4 L323.528 207.4 L323.528 238.025 L318.829 238.025 L318.829 207.4 L306.56 207.4 L306.56 203.465 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M354.245 223.997 L354.245 226.08 L334.662 226.08 Q334.94 230.478 337.301 232.793 Q339.685 235.085 343.921 235.085 Q346.375 235.085 348.667 234.483 Q350.981 233.881 353.25 232.677 L353.25 236.705 Q350.958 237.677 348.551 238.187 Q346.143 238.696 343.667 238.696 Q337.463 238.696 333.829 235.085 Q330.218 231.474 330.218 225.316 Q330.218 218.951 333.643 215.224 Q337.092 211.474 342.926 211.474 Q348.157 211.474 351.19 214.853 Q354.245 218.21 354.245 223.997 M349.986 222.747 Q349.94 219.252 348.018 217.168 Q346.12 215.085 342.972 215.085 Q339.407 215.085 337.255 217.099 Q335.125 219.113 334.801 222.77 L349.986 222.747 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M377.764 212.863 L377.764 216.891 Q375.958 215.965 374.014 215.502 Q372.069 215.039 369.986 215.039 Q366.815 215.039 365.217 216.011 Q363.643 216.983 363.643 218.928 Q363.643 220.409 364.778 221.265 Q365.912 222.099 369.338 222.863 L370.796 223.187 Q375.333 224.159 377.231 225.941 Q379.152 227.701 379.152 230.872 Q379.152 234.483 376.282 236.589 Q373.435 238.696 368.435 238.696 Q366.352 238.696 364.083 238.279 Q361.838 237.886 359.338 237.076 L359.338 232.677 Q361.699 233.904 363.99 234.529 Q366.282 235.131 368.528 235.131 Q371.537 235.131 373.157 234.113 Q374.777 233.071 374.777 231.196 Q374.777 229.46 373.597 228.534 Q372.44 227.608 368.481 226.752 L367 226.404 Q363.041 225.571 361.282 223.858 Q359.523 222.122 359.523 219.113 Q359.523 215.455 362.116 213.465 Q364.708 211.474 369.477 211.474 Q371.838 211.474 373.921 211.821 Q376.004 212.168 377.764 212.863 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M390.148 204.738 L390.148 212.099 L398.921 212.099 L398.921 215.409 L390.148 215.409 L390.148 229.483 Q390.148 232.654 391.004 233.557 Q391.884 234.46 394.546 234.46 L398.921 234.46 L398.921 238.025 L394.546 238.025 Q389.615 238.025 387.74 236.196 Q385.865 234.344 385.865 229.483 L385.865 215.409 L382.74 215.409 L382.74 212.099 L385.865 212.099 L385.865 204.738 L390.148 204.738 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip510)\" style=\"stroke:#e26f46; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  138.56,272.585 282.56,272.585 \n  \"/>\n<path clip-path=\"url(#clip510)\" d=\"M306.56 255.305 L335.796 255.305 L335.796 259.24 L323.528 259.24 L323.528 289.865 L318.829 289.865 L318.829 259.24 L306.56 259.24 L306.56 255.305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M348.18 267.92 Q347.463 267.504 346.606 267.318 Q345.773 267.11 344.754 267.11 Q341.143 267.11 339.199 269.471 Q337.278 271.809 337.278 276.207 L337.278 289.865 L332.995 289.865 L332.995 263.939 L337.278 263.939 L337.278 267.967 Q338.62 265.606 340.773 264.471 Q342.926 263.314 346.004 263.314 Q346.444 263.314 346.977 263.383 Q347.509 263.43 348.157 263.545 L348.18 267.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M364.43 276.832 Q359.268 276.832 357.278 278.013 Q355.287 279.193 355.287 282.041 Q355.287 284.309 356.768 285.652 Q358.273 286.971 360.842 286.971 Q364.384 286.971 366.514 284.471 Q368.666 281.948 368.666 277.781 L368.666 276.832 L364.43 276.832 M372.926 275.073 L372.926 289.865 L368.666 289.865 L368.666 285.929 Q367.208 288.291 365.032 289.425 Q362.856 290.536 359.708 290.536 Q355.727 290.536 353.366 288.314 Q351.028 286.068 351.028 282.318 Q351.028 277.943 353.944 275.721 Q356.884 273.499 362.694 273.499 L368.666 273.499 L368.666 273.082 Q368.666 270.143 366.722 268.545 Q364.801 266.925 361.305 266.925 Q359.083 266.925 356.977 267.457 Q354.87 267.99 352.926 269.055 L352.926 265.119 Q355.264 264.217 357.463 263.777 Q359.662 263.314 361.745 263.314 Q367.37 263.314 370.148 266.231 Q372.926 269.147 372.926 275.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M381.699 263.939 L385.958 263.939 L385.958 289.865 L381.699 289.865 L381.699 263.939 M381.699 253.846 L385.958 253.846 L385.958 259.24 L381.699 259.24 L381.699 253.846 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip510)\" d=\"M416.421 274.217 L416.421 289.865 L412.162 289.865 L412.162 274.355 Q412.162 270.675 410.726 268.846 Q409.291 267.018 406.421 267.018 Q402.972 267.018 400.981 269.217 Q398.99 271.416 398.99 275.212 L398.99 289.865 L394.708 289.865 L394.708 263.939 L398.99 263.939 L398.99 267.967 Q400.518 265.629 402.578 264.471 Q404.662 263.314 407.37 263.314 Q411.837 263.314 414.129 266.092 Q416.421 268.846 416.421 274.217 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_resultTest = save_resultTest ./ 10;\n",
    "save_resultTrain = save_resultTrain ./ 10;\n",
    "\n",
    "plot(log.(1:epochs), save_resultTest,label=\"Test\")\n",
    "plot!(log.(1:epochs), save_resultTrain,label = \"Train\", title = \"Accuracy\", legend = :outertopleft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrix for the training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0' and positive='1'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase C:\\Users\\xkzmx\\.julia\\packages\\MLJBase\\pCiRR\\src\\measures\\confusion_matrix.jl:112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              ┌───────────────────────────┐\n",
       "              │       Ground Truth        │\n",
       "┌─────────────┼─────────────┬─────────────┤\n",
       "│  Predicted  │      0      │      1      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      0      │     270     │     493     │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      1      │     398     │     227     │\n",
       "└─────────────┴─────────────┴─────────────┘\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_x_train_result = []\n",
    "for i  = 1:size(x_train,2)\n",
    "    if softmax(model(x_train[:,i]))[1]>0.5\n",
    "        push!(simplified_x_train_result, 0)\n",
    "    else\n",
    "        push!(simplified_x_train_result, 1)\n",
    "    end\n",
    "end\n",
    "simplified_y_train = []\n",
    "for i  = 1:size(x_train,2)\n",
    "    if y_train[i] == 0\n",
    "        push!(simplified_y_train, 0)\n",
    "    else\n",
    "        push!(simplified_y_train, 1)\n",
    "    end\n",
    "end\n",
    "\n",
    "# ConfusionMatrix for the training data\n",
    "print(\"ConfusionMatrix for the training data\\n\")\n",
    "ConfusionMatrix()(simplified_x_train_result, simplified_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfusionMatrix for the test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0' and positive='1'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase C:\\Users\\xkzmx\\.julia\\packages\\MLJBase\\pCiRR\\src\\measures\\confusion_matrix.jl:112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              ┌───────────────────────────┐\n",
       "              │       Ground Truth        │\n",
       "┌─────────────┼─────────────┬─────────────┤\n",
       "│  Predicted  │      0      │      1      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      0      │     127     │     183     │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      1      │     202     │     84      │\n",
       "└─────────────┴─────────────┴─────────────┘\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_x_test_result = []\n",
    "for i  = 1:size(x_test,2) \n",
    "    if softmax(model(x_test[:,i]))[1]>0.5\n",
    "        push!(simplified_x_test_result, 0)\n",
    "    else\n",
    "        push!(simplified_x_test_result, 1)\n",
    "    end\n",
    "end\n",
    "simplified_y_test = []\n",
    "for i  = 1:size(x_test,2) \n",
    "    if y_test[i] == 0\n",
    "        push!(simplified_y_test, 0)\n",
    "    else\n",
    "        push!(simplified_y_test, 1)\n",
    "    end\n",
    "end\n",
    "\n",
    "# ConfusionMatrix for the test data\n",
    "print(\"ConfusionMatrix for the test data\\n\")\n",
    "ConfusionMatrix()(simplified_x_test_result, simplified_y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

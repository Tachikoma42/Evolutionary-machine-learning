{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sihao Ren // sren03 // 947105531\n",
    "using Flux\n",
    "using Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated\n",
    "using Flux: @epochs\n",
    "using MLJBase\n",
    "using Printf\n",
    "using BSON: @save # for save weights\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(CSV.File(\"data/nba_logreg.csv\"));\n",
    "zer = 0\n",
    "on = 0\n",
    "for r in eachrow(df)\n",
    "    if r.TARGET_5Yrs == 0\n",
    "        zer = zer + 1\n",
    "    else\n",
    "        on = on + 1\n",
    "    end\n",
    "end\n",
    "increase = on-zer\n",
    "\n",
    "for r in eachrow(df)\n",
    "    if r.TARGET_5Yrs == 0 && increase > 0\n",
    "        push!(df, r)\n",
    "        increase = increase - 1\n",
    "    end\n",
    "        \n",
    "end\n",
    "df[shuffle(axes(df, 1)), :]; #https://github.com/JuliaData/DataFrames.jl/issues/2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: readdlm not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: readdlm not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[66]:1",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "rawdata = readdlm(\"data/nba_logreg.csv\")'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[1:1100, :];\n",
    "test = df[1101:1662, :];\n",
    "y_train, y_test = onehotbatch(train[:, end], 0:1), onehotbatch(test[:, end], 0:1)\n",
    "\n",
    "x_train = train[:, 1:end-1];\n",
    "x_test = test[:, 1:end-1];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>1,100 rows Ã— 19 columns (omitted printing of 10 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>GP</th><th>MIN</th><th>PTS</th><th>FGM</th><th>FGA</th><th>FG%</th><th>3P Made</th><th>3PA</th><th>3P%</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Union{Missing, Float64}\">Float64?</th></tr></thead><tbody><tr><th>1</th><td>36</td><td>27.4</td><td>7.4</td><td>2.6</td><td>7.6</td><td>34.7</td><td>0.5</td><td>2.1</td><td>25.0</td></tr><tr><th>2</th><td>35</td><td>26.9</td><td>7.2</td><td>2.0</td><td>6.7</td><td>29.6</td><td>0.7</td><td>2.8</td><td>23.5</td></tr><tr><th>3</th><td>74</td><td>15.3</td><td>5.2</td><td>2.0</td><td>4.7</td><td>42.2</td><td>0.4</td><td>1.7</td><td>24.4</td></tr><tr><th>4</th><td>58</td><td>11.6</td><td>5.7</td><td>2.3</td><td>5.5</td><td>42.6</td><td>0.1</td><td>0.5</td><td>22.6</td></tr><tr><th>5</th><td>48</td><td>11.5</td><td>4.5</td><td>1.6</td><td>3.0</td><td>52.4</td><td>0.0</td><td>0.1</td><td>0.0</td></tr><tr><th>6</th><td>75</td><td>11.4</td><td>3.7</td><td>1.5</td><td>3.5</td><td>42.3</td><td>0.3</td><td>1.1</td><td>32.5</td></tr><tr><th>7</th><td>62</td><td>10.9</td><td>6.6</td><td>2.5</td><td>5.8</td><td>43.5</td><td>0.0</td><td>0.1</td><td>50.0</td></tr><tr><th>8</th><td>48</td><td>10.3</td><td>5.7</td><td>2.3</td><td>5.4</td><td>41.5</td><td>0.4</td><td>1.5</td><td>30.0</td></tr><tr><th>9</th><td>65</td><td>9.9</td><td>2.4</td><td>1.0</td><td>2.4</td><td>39.2</td><td>0.1</td><td>0.5</td><td>23.3</td></tr><tr><th>10</th><td>42</td><td>8.5</td><td>3.7</td><td>1.4</td><td>3.5</td><td>38.3</td><td>0.1</td><td>0.3</td><td>21.4</td></tr><tr><th>11</th><td>35</td><td>6.9</td><td>2.3</td><td>0.9</td><td>2.4</td><td>36.5</td><td>0.0</td><td>0.1</td><td>33.3</td></tr><tr><th>12</th><td>40</td><td>6.7</td><td>3.6</td><td>1.2</td><td>3.0</td><td>39.8</td><td>0.1</td><td>0.6</td><td>13.6</td></tr><tr><th>13</th><td>27</td><td>6.6</td><td>1.3</td><td>0.6</td><td>1.3</td><td>47.2</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>14</th><td>45</td><td>15.3</td><td>5.6</td><td>1.9</td><td>6.0</td><td>32.3</td><td>1.1</td><td>3.6</td><td>30.1</td></tr><tr><th>15</th><td>44</td><td>6.4</td><td>2.4</td><td>1.0</td><td>1.9</td><td>53.7</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>16</th><td>40</td><td>6.1</td><td>2.6</td><td>0.9</td><td>1.8</td><td>51.4</td><td>0.1</td><td>0.4</td><td>14.3</td></tr><tr><th>17</th><td>49</td><td>5.3</td><td>2.1</td><td>0.7</td><td>1.9</td><td>37.6</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>18</th><td>41</td><td>4.2</td><td>1.7</td><td>0.6</td><td>1.6</td><td>34.8</td><td>0.1</td><td>0.3</td><td>21.4</td></tr><tr><th>19</th><td>82</td><td>37.2</td><td>19.2</td><td>7.5</td><td>15.3</td><td>49.0</td><td>0.1</td><td>0.3</td><td>22.7</td></tr><tr><th>20</th><td>82</td><td>37.2</td><td>19.2</td><td>7.5</td><td>15.3</td><td>49.0</td><td>0.1</td><td>0.3</td><td>22.7</td></tr><tr><th>21</th><td>80</td><td>31.4</td><td>14.3</td><td>5.9</td><td>11.1</td><td>52.5</td><td>0.0</td><td>0.1</td><td>11.1</td></tr><tr><th>22</th><td>82</td><td>30.5</td><td>13.3</td><td>5.4</td><td>11.0</td><td>48.9</td><td>0.0</td><td>0.1</td><td>16.7</td></tr><tr><th>23</th><td>76</td><td>30.3</td><td>10.6</td><td>4.4</td><td>11.7</td><td>37.5</td><td>0.1</td><td>0.4</td><td>13.3</td></tr><tr><th>24</th><td>61</td><td>29.6</td><td>12.0</td><td>4.9</td><td>10.7</td><td>45.4</td><td>0.7</td><td>2.0</td><td>32.0</td></tr><tr><th>25</th><td>32</td><td>15.2</td><td>6.3</td><td>2.8</td><td>5.2</td><td>53.3</td><td>0.0</td><td>0.1</td><td>0.0</td></tr><tr><th>26</th><td>76</td><td>29.3</td><td>10.4</td><td>4.0</td><td>7.8</td><td>51.4</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>27</th><td>52</td><td>24.6</td><td>9.3</td><td>3.1</td><td>6.8</td><td>45.1</td><td>1.1</td><td>2.6</td><td>43.4</td></tr><tr><th>28</th><td>76</td><td>22.5</td><td>8.8</td><td>3.8</td><td>9.2</td><td>41.5</td><td>0.0</td><td>0.1</td><td>0.0</td></tr><tr><th>29</th><td>78</td><td>22.0</td><td>10.1</td><td>3.9</td><td>8.7</td><td>44.7</td><td>0.7</td><td>1.8</td><td>39.0</td></tr><tr><th>30</th><td>51</td><td>20.7</td><td>8.4</td><td>3.2</td><td>6.4</td><td>49.7</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& GP & MIN & PTS & FGM & FGA & FG\\% & 3P Made & 3PA & 3P\\% & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64? & \\\\\n",
       "\t\\hline\n",
       "\t1 & 36 & 27.4 & 7.4 & 2.6 & 7.6 & 34.7 & 0.5 & 2.1 & 25.0 & $\\dots$ \\\\\n",
       "\t2 & 35 & 26.9 & 7.2 & 2.0 & 6.7 & 29.6 & 0.7 & 2.8 & 23.5 & $\\dots$ \\\\\n",
       "\t3 & 74 & 15.3 & 5.2 & 2.0 & 4.7 & 42.2 & 0.4 & 1.7 & 24.4 & $\\dots$ \\\\\n",
       "\t4 & 58 & 11.6 & 5.7 & 2.3 & 5.5 & 42.6 & 0.1 & 0.5 & 22.6 & $\\dots$ \\\\\n",
       "\t5 & 48 & 11.5 & 4.5 & 1.6 & 3.0 & 52.4 & 0.0 & 0.1 & 0.0 & $\\dots$ \\\\\n",
       "\t6 & 75 & 11.4 & 3.7 & 1.5 & 3.5 & 42.3 & 0.3 & 1.1 & 32.5 & $\\dots$ \\\\\n",
       "\t7 & 62 & 10.9 & 6.6 & 2.5 & 5.8 & 43.5 & 0.0 & 0.1 & 50.0 & $\\dots$ \\\\\n",
       "\t8 & 48 & 10.3 & 5.7 & 2.3 & 5.4 & 41.5 & 0.4 & 1.5 & 30.0 & $\\dots$ \\\\\n",
       "\t9 & 65 & 9.9 & 2.4 & 1.0 & 2.4 & 39.2 & 0.1 & 0.5 & 23.3 & $\\dots$ \\\\\n",
       "\t10 & 42 & 8.5 & 3.7 & 1.4 & 3.5 & 38.3 & 0.1 & 0.3 & 21.4 & $\\dots$ \\\\\n",
       "\t11 & 35 & 6.9 & 2.3 & 0.9 & 2.4 & 36.5 & 0.0 & 0.1 & 33.3 & $\\dots$ \\\\\n",
       "\t12 & 40 & 6.7 & 3.6 & 1.2 & 3.0 & 39.8 & 0.1 & 0.6 & 13.6 & $\\dots$ \\\\\n",
       "\t13 & 27 & 6.6 & 1.3 & 0.6 & 1.3 & 47.2 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t14 & 45 & 15.3 & 5.6 & 1.9 & 6.0 & 32.3 & 1.1 & 3.6 & 30.1 & $\\dots$ \\\\\n",
       "\t15 & 44 & 6.4 & 2.4 & 1.0 & 1.9 & 53.7 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t16 & 40 & 6.1 & 2.6 & 0.9 & 1.8 & 51.4 & 0.1 & 0.4 & 14.3 & $\\dots$ \\\\\n",
       "\t17 & 49 & 5.3 & 2.1 & 0.7 & 1.9 & 37.6 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t18 & 41 & 4.2 & 1.7 & 0.6 & 1.6 & 34.8 & 0.1 & 0.3 & 21.4 & $\\dots$ \\\\\n",
       "\t19 & 82 & 37.2 & 19.2 & 7.5 & 15.3 & 49.0 & 0.1 & 0.3 & 22.7 & $\\dots$ \\\\\n",
       "\t20 & 82 & 37.2 & 19.2 & 7.5 & 15.3 & 49.0 & 0.1 & 0.3 & 22.7 & $\\dots$ \\\\\n",
       "\t21 & 80 & 31.4 & 14.3 & 5.9 & 11.1 & 52.5 & 0.0 & 0.1 & 11.1 & $\\dots$ \\\\\n",
       "\t22 & 82 & 30.5 & 13.3 & 5.4 & 11.0 & 48.9 & 0.0 & 0.1 & 16.7 & $\\dots$ \\\\\n",
       "\t23 & 76 & 30.3 & 10.6 & 4.4 & 11.7 & 37.5 & 0.1 & 0.4 & 13.3 & $\\dots$ \\\\\n",
       "\t24 & 61 & 29.6 & 12.0 & 4.9 & 10.7 & 45.4 & 0.7 & 2.0 & 32.0 & $\\dots$ \\\\\n",
       "\t25 & 32 & 15.2 & 6.3 & 2.8 & 5.2 & 53.3 & 0.0 & 0.1 & 0.0 & $\\dots$ \\\\\n",
       "\t26 & 76 & 29.3 & 10.4 & 4.0 & 7.8 & 51.4 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t27 & 52 & 24.6 & 9.3 & 3.1 & 6.8 & 45.1 & 1.1 & 2.6 & 43.4 & $\\dots$ \\\\\n",
       "\t28 & 76 & 22.5 & 8.8 & 3.8 & 9.2 & 41.5 & 0.0 & 0.1 & 0.0 & $\\dots$ \\\\\n",
       "\t29 & 78 & 22.0 & 10.1 & 3.9 & 8.7 & 44.7 & 0.7 & 1.8 & 39.0 & $\\dots$ \\\\\n",
       "\t30 & 51 & 20.7 & 8.4 & 3.2 & 6.4 & 49.7 & 0.0 & 0.0 & 0.0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1100Ã—19 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0mâ”‚\u001b[1m GP    \u001b[0m\u001b[1m MIN     \u001b[0m\u001b[1m PTS     \u001b[0m\u001b[1m FGM     \u001b[0m\u001b[1m FGA     \u001b[0m\u001b[1m FG%     \u001b[0m\u001b[1m 3P Made \u001b[0m\u001b[1m 3PA     \u001b[0m\u001b[1m \u001b[0m â‹¯\n",
       "\u001b[1m      \u001b[0mâ”‚\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m \u001b[0m â‹¯\n",
       "â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "    1 â”‚    36     27.4      7.4      2.6      7.6     34.7      0.5      2.1   â‹¯\n",
       "    2 â”‚    35     26.9      7.2      2.0      6.7     29.6      0.7      2.8\n",
       "    3 â”‚    74     15.3      5.2      2.0      4.7     42.2      0.4      1.7\n",
       "    4 â”‚    58     11.6      5.7      2.3      5.5     42.6      0.1      0.5\n",
       "    5 â”‚    48     11.5      4.5      1.6      3.0     52.4      0.0      0.1   â‹¯\n",
       "    6 â”‚    75     11.4      3.7      1.5      3.5     42.3      0.3      1.1\n",
       "    7 â”‚    62     10.9      6.6      2.5      5.8     43.5      0.0      0.1\n",
       "    8 â”‚    48     10.3      5.7      2.3      5.4     41.5      0.4      1.5\n",
       "    9 â”‚    65      9.9      2.4      1.0      2.4     39.2      0.1      0.5   â‹¯\n",
       "   10 â”‚    42      8.5      3.7      1.4      3.5     38.3      0.1      0.3\n",
       "   11 â”‚    35      6.9      2.3      0.9      2.4     36.5      0.0      0.1\n",
       "  â‹®   â”‚   â‹®       â‹®        â‹®        â‹®        â‹®        â‹®        â‹®        â‹®      â‹±\n",
       " 1091 â”‚    48     34.0     16.5      5.9     13.5     43.9      1.8      4.3\n",
       " 1092 â”‚    82     30.4      8.9      3.7      8.6     42.5      0.1      0.5   â‹¯\n",
       " 1093 â”‚    50     31.2     10.9      4.3      9.2     46.5      1.4      3.3\n",
       " 1094 â”‚    28     30.4      6.3      2.4      4.8     50.0      0.0      0.0\n",
       " 1095 â”‚    41     15.2      3.7      1.4      3.8     35.7      0.1      0.3\n",
       " 1096 â”‚    49     29.7      9.9      3.5      8.3     42.5      1.1      3.0   â‹¯\n",
       " 1097 â”‚    50     25.8     10.1      3.6      8.1     44.9      0.7      1.9\n",
       " 1098 â”‚    47     22.5      9.6      3.8      8.4     45.2      0.1      0.2\n",
       " 1099 â”‚    50     22.3      8.2      3.0      6.4     46.3      0.2      0.5\n",
       " 1100 â”‚    25     29.5     12.3      5.2     10.5     49.0      0.2      0.6   â‹¯\n",
       "\u001b[36m                                                11 columns and 1079 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = transpose(Matrix(x_train))\n",
    "x_test = transpose(Matrix(x_test));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Union{Missing, Float64}[50.0 49.0 â€¦ 26.0 38.0; 19.8 16.0 â€¦ 10.3 8.5; â€¦ ; 0.3 0.9 â€¦ 0.4 0.4; 1.4 0.9 â€¦ 0.5 0.5], Bool[0 0 â€¦ 1 1; 1 1 â€¦ 0 0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(19, 38, Ïƒ),                     \u001b[90m# 760 parameters\u001b[39m\n",
       "  Dense(38, 2),                         \u001b[90m# 78 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ")\u001b[90m                   # Total: 4 arrays, \u001b[39m838 parameters, 3.523 KiB."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(Dense(19, 2*19, sigmoid), Dense(2*19, 2), softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myaccuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x, y) = crossentropy(model(x), y) \n",
    "\n",
    "optim = ADAM()\n",
    "dataset = repeated((x_train,y_train),1);\n",
    "myaccuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching (::Dense{typeof(Ïƒ), Matrix{Float32}, Vector{Float32}})(::Float64)\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Dense)(\u001b[91m::AbstractVecOrMat{T} where T\u001b[39m) at C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\layers\\basic.jl:156\n\u001b[0m  (::Dense)(\u001b[91m::AbstractArray\u001b[39m) at C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\layers\\basic.jl:161",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::Dense{typeof(Ïƒ), Matrix{Float32}, Vector{Float32}})(::Float64)\n\u001b[0mClosest candidates are:\n\u001b[0m  (::Dense)(\u001b[91m::AbstractVecOrMat{T} where T\u001b[39m) at C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\layers\\basic.jl:156\n\u001b[0m  (::Dense)(\u001b[91m::AbstractArray\u001b[39m) at C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\layers\\basic.jl:161",
      "",
      "Stacktrace:",
      "  [1] macro expansion",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface2.jl:0 [inlined]",
      "  [2] _pullback(ctx::Zygote.Context, f::Dense{typeof(Ïƒ), Matrix{Float32}, Vector{Float32}}, args::Float64)",
      "    @ Zygote C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface2.jl:9",
      "  [3] _pullback",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\layers\\basic.jl:47 [inlined]",
      "  [4] _pullback(::Zygote.Context, ::typeof(Flux.applychain), ::Tuple{Dense{typeof(Ïƒ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}, ::Float64)",
      "    @ Zygote C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface2.jl:0",
      "  [5] _pullback",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\layers\\basic.jl:49 [inlined]",
      "  [6] _pullback(ctx::Zygote.Context, f::Chain{Tuple{Dense{typeof(Ïƒ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, typeof(softmax)}}, args::Float64)",
      "    @ Zygote C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface2.jl:0",
      "  [7] _pullback",
      "    @ .\\In[88]:1 [inlined]",
      "  [8] _pullback(::Zygote.Context, ::typeof(loss), ::Float64, ::Bool)",
      "    @ Zygote C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface2.jl:0",
      "  [9] _apply",
      "    @ .\\boot.jl:814 [inlined]",
      " [10] adjoint",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\lib\\lib.jl:200 [inlined]",
      " [11] _pullback",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\ZygoteRules\\AIbCs\\src\\adjoint.jl:65 [inlined]",
      " [12] _pullback",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\optimise\\train.jl:110 [inlined]",
      " [13] _pullback(::Zygote.Context, ::Flux.Optimise.var\"#39#45\"{typeof(loss), Tuple{Float64, Bool}})",
      "    @ Zygote C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface2.jl:0",
      " [14] pullback(f::Function, ps::Zygote.Params)",
      "    @ Zygote C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface.jl:352",
      " [15] gradient(f::Function, args::Zygote.Params)",
      "    @ Zygote C:\\Users\\xkzmx\\.julia\\packages\\Zygote\\FPUm3\\src\\compiler\\interface.jl:75",
      " [16] macro expansion",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\optimise\\train.jl:109 [inlined]",
      " [17] macro expansion",
      "    @ C:\\Users\\xkzmx\\.julia\\packages\\Juno\\n6wyj\\src\\progress.jl:134 [inlined]",
      " [18] train!(loss::Function, ps::Zygote.Params, data::Base.Iterators.Zip{Tuple{LinearAlgebra.Transpose{Union{Missing, Float64}, Matrix{Union{Missing, Float64}}}, Flux.OneHotArray{UInt32, 2, 1, 2, Vector{UInt32}}}}, opt::ADAM; cb::Flux.Optimise.var\"#40#46\")",
      "    @ Flux.Optimise C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\optimise\\train.jl:107",
      " [19] train!(loss::Function, ps::Zygote.Params, data::Base.Iterators.Zip{Tuple{LinearAlgebra.Transpose{Union{Missing, Float64}, Matrix{Union{Missing, Float64}}}, Flux.OneHotArray{UInt32, 2, 1, 2, Vector{UInt32}}}}, opt::ADAM)",
      "    @ Flux.Optimise C:\\Users\\xkzmx\\.julia\\packages\\Flux\\qAdFM\\src\\optimise\\train.jl:105",
      " [20] top-level scope",
      "    @ In[89]:1",
      " [21] eval",
      "    @ .\\boot.jl:373 [inlined]",
      " [22] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "Flux.train!(loss, Flux.params(model), zip(x_train, y_train), optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
